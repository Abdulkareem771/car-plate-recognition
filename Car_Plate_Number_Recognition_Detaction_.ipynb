{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ðŸš— Car License Plate Recognition System\n",
        "## Professional Analysis, Implementation, and Usage Guide\n",
        "\n",
        "### Team Members\n",
        "- **Ahmed Al-Duais** - 202270176\n",
        "- **Abulkareem Thiab** - 202270136\n",
        "- **Ayman Mrwan** - 202270324\n",
        "\n",
        "---\n",
        "\n",
        "### Table of Contents\n",
        "1. Project Overview\n",
        "2. System Architecture\n",
        "3. Setup and Model Assets\n",
        "4. GUI Applications and Usage\n",
        "5. Data Preparation\n",
        "6. Model Training\n",
        "7. Inference and Testing\n",
        "8. Real-time Detection\n",
        "9. Results and Performance\n",
        "10. Future Improvements\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1) Project Overview\n",
        "\n",
        "This project implements an end-to-end car license plate detection and recognition system tailored for Yemeni plates. It uses YOLOv8 for plate detection and EasyOCR for Arabic/English text recognition, with multiple GUIs for real-time and batch workflows.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2) System Architecture\n",
        "\n",
        "```mermaid\n",
        "graph TD\n",
        "    A[Input Images/Video] --> B[Data Preprocessing]\n",
        "    B --> C[YOLOv8 Model Training]\n",
        "    C --> D[Model Validation]\n",
        "    D --> E[Inference Engine]\n",
        "    E --> F[Real-time Detection]\n",
        "    F --> G[Output Results]\n",
        "    \n",
        "    H[Live Stream] --> I[Frame Processing]\n",
        "    I --> E\n",
        "    \n",
        "    J[CSV Annotations] --> K[YOLO Format Conversion]\n",
        "    K --> B\n",
        "```\n",
        "\n",
        "Core Modules:\n",
        "- Data pipeline: `convert_to_yolo.py`\n",
        "- Training: `main.py`, `yemen car plate number model train.py`\n",
        "- Inference/Utilities: `model_test.py`, `seperated-plates.py`\n",
        "- Real-time: `recognition-with-live-stream/live-stream.py`\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3) Setup and Model Assets\n",
        "\n",
        "- Python 3.13.5+\n",
        "- Install dependencies (UV recommended):\n",
        "```bash\n",
        "pip install uv\n",
        "uv sync\n",
        "```\n",
        "- Verify:\n",
        "```bash\n",
        "python -c \"import ultralytics, easyocr, cv2; print('OK')\"\n",
        "```\n",
        "\n",
        "Models:\n",
        "- Primary plate detector: `src/models/yolov8s14/weights/best.pt`\n",
        "- Component detector (text/city/number): `src/models/yolov8s5/weights/best.pt`\n",
        "- Base models: `yolo11n.pt`, `yolo11s.pt`, `yolov8n.pt`, `yolov8s.pt`\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4) GUI Applications and Usage\n",
        "\n",
        "- Advanced Qt GUI (tabs + live stream):\n",
        "```bash\n",
        "python GUIs/qt_with_live/main.py\n",
        "```\n",
        "- Simple Qt GUI (single image):\n",
        "```bash\n",
        "python GUIs/simple-gui-qt.py\n",
        "```\n",
        "- Twoâ€‘Stage GUI:\n",
        "```bash\n",
        "python yemen_plate_gui.py\n",
        "```\n",
        "- Enhanced Twoâ€‘Stage GUI (latest):\n",
        "```bash\n",
        "python yemen_plate_v2_gui.py\n",
        "```\n",
        "- Simple Tkinter GUI:\n",
        "```bash\n",
        "python GUIs/simple_gui_tiknter/simple_gui_tiknter.py\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Environment and project scan\n",
        "import os, sys\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "print(\"ðŸ“ Project Structure (top-level):\")\n",
        "for path in sorted(Path('.').iterdir()):\n",
        "    if path.name.startswith('.'):\n",
        "        continue\n",
        "    print('-', path)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5) Data Preparation\n",
        "\n",
        "Analyze dataset structure and distribution for both datasets.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Dataset analysis utilities\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def analyze_dataset_structure():\n",
        "    datasets = {\n",
        "        \"Yemen Car Plate v1i\": \"src/data/plat number car yemen.v1i.yolov8\",\n",
        "        \"Yemen Plate\": \"src/data/yemen-plate\"\n",
        "    }\n",
        "    results = {}\n",
        "    for name, path in datasets.items():\n",
        "        if os.path.exists(path):\n",
        "            train_images = len([f for f in os.listdir(f\"{path}/train/images\") if f.lower().endswith(('.jpg','.jpeg','.png'))])\n",
        "            val_images = len([f for f in os.listdir(f\"{path}/valid/images\") if f.lower().endswith(('.jpg','.jpeg','.png'))])\n",
        "            test_images = len([f for f in os.listdir(f\"{path}/test/images\") if f.lower().endswith(('.jpg','.jpeg','.png'))])\n",
        "            results[name] = {\n",
        "                'train': train_images,\n",
        "                'validation': val_images,\n",
        "                'test': test_images,\n",
        "                'total': train_images + val_images + test_images\n",
        "            }\n",
        "    return results\n",
        "\n",
        "def visualize_dataset_stats(stats):\n",
        "    if not stats:\n",
        "        print('No dataset stats available')\n",
        "        return\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
        "    datasets = list(stats.keys())\n",
        "    train_counts = [stats[ds]['train'] for ds in datasets]\n",
        "    val_counts = [stats[ds]['validation'] for ds in datasets]\n",
        "    test_counts = [stats[ds]['test'] for ds in datasets]\n",
        "    x = np.arange(len(datasets)); width = 0.25\n",
        "    ax1.bar(x - width, train_counts, width, label='Train', alpha=0.8)\n",
        "    ax1.bar(x, val_counts, width, label='Validation', alpha=0.8)\n",
        "    ax1.bar(x + width, test_counts, width, label='Test', alpha=0.8)\n",
        "    ax1.set_xlabel('Dataset'); ax1.set_ylabel('Number of Images'); ax1.set_title('Dataset Distribution Comparison')\n",
        "    ax1.set_xticks(x); ax1.set_xticklabels(datasets, rotation=45); ax1.legend(); ax1.grid(True, alpha=0.3)\n",
        "    primary = \"Yemen Car Plate v1i\"\n",
        "    if primary in stats:\n",
        "        sizes = [stats[primary]['train'], stats[primary]['validation'], stats[primary]['test']]\n",
        "        labels = ['Train','Validation','Test']\n",
        "        ax2.pie(sizes, labels=labels, autopct='%1.1f%%', startangle=90)\n",
        "        ax2.set_title(f'{primary} Split Distribution')\n",
        "    plt.tight_layout(); plt.show()\n",
        "\n",
        "stats = analyze_dataset_structure()\n",
        "visualize_dataset_stats(stats)\n",
        "print(stats)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6) Model Training\n",
        "\n",
        "Train plate and component models with YOLOv8.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example training snippets (commented for safety)\n",
        "# from ultralytics import YOLO\n",
        "# model = YOLO('yolov8s.pt')\n",
        "# results = model.train(\n",
        "#     data='src/data/yemen-plate/data.yaml',\n",
        "#     epochs=50,\n",
        "#     imgsz=640,\n",
        "#     batch=9,\n",
        "#     device=0,\n",
        "#     name='yolov8s',\n",
        "#     workers=0,\n",
        "# )\n",
        "\n",
        "print('Refer to main.py or yemen car plate number model train.py for full training scripts.')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7) Inference and Testing\n",
        "\n",
        "Run demos: basic detection + OCR, and two-stage detection + enhanced OCR.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Basic detection + OCR demo (from Project notebook)\n",
        "from pathlib import Path\n",
        "import cv2\n",
        "from ultralytics import YOLO\n",
        "import easyocr\n",
        "\n",
        "primary_model = Path('src/models/yolov8s14/weights/best.pt')\n",
        "example_dir = Path('src/data/plat number car yemen.v1i.yolov8/test/images')\n",
        "\n",
        "if primary_model.exists():\n",
        "    model = YOLO(str(primary_model))\n",
        "    reader = easyocr.Reader(['en','ar'])\n",
        "    img_path = None\n",
        "    if example_dir.exists():\n",
        "        for p in example_dir.iterdir():\n",
        "            if p.suffix.lower() in {'.jpg','.jpeg','.png'}:\n",
        "                img_path = p; break\n",
        "    if img_path:\n",
        "        img = cv2.imread(str(img_path))\n",
        "        results = model(img)\n",
        "        for r in results:\n",
        "            if r.boxes is None: continue\n",
        "            for box in r.boxes:\n",
        "                x1, y1, x2, y2 = map(int, box.xyxy[0].cpu().numpy())\n",
        "                crop = img[y1:y2, x1:x2]\n",
        "                if crop.size == 0: continue\n",
        "                ocr = reader.readtext(crop)\n",
        "                text = \" \".join([t[1] for t in ocr])\n",
        "                if 'Ø®ØµÙˆØµÙŠ' in text or 'Ù†Ù‚Ù„' in text:\n",
        "                    text = 'Ø®ØµÙˆØµÙŠ Ù†Ù‚Ù„ Ø§Ø¬Ø±Ø©'\n",
        "                print('Detected:', text)\n",
        "    else:\n",
        "        print('No example images found.')\n",
        "else:\n",
        "    print('Model not found:', primary_model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Two-stage detection + enhanced OCR demo (from Project notebook)\n",
        "from pathlib import Path\n",
        "import cv2, numpy as np\n",
        "from ultralytics import YOLO\n",
        "import easyocr\n",
        "\n",
        "def arabic_to_english_digits(text: str) -> str:\n",
        "    return text.translate(str.maketrans(\"Ù Ù¡Ù¢Ù£Ù¤Ù¥Ù¦Ù§Ù¨Ù©\", \"0123456789\"))\n",
        "\n",
        "def preprocess_for_arabic_ocr(image):\n",
        "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "    h, w = gray.shape\n",
        "    if h < 50 or w < 100:\n",
        "        sf = max(50/h, 100/w, 2.0)\n",
        "        gray = cv2.resize(gray, (int(w*sf), int(h*sf)), interpolation=cv2.INTER_CUBIC)\n",
        "    blurred = cv2.GaussianBlur(gray, (3,3), 0)\n",
        "    clahe = cv2.createCLAHE(clipLimit=4.0, tileGridSize=(8,8))\n",
        "    enhanced = clahe.apply(blurred)\n",
        "    filtered = cv2.bilateralFilter(enhanced, 9, 75, 75)\n",
        "    kernel = np.array([[-1,-1,-1],[-1,9,-1],[-1,-1,-1]])\n",
        "    sharpened = cv2.filter2D(filtered, -1, kernel)\n",
        "    thresh = cv2.adaptiveThreshold(sharpened, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 11, 2)\n",
        "    return cv2.cvtColor(thresh, cv2.COLOR_GRAY2BGR)\n",
        "\n",
        "def extract_text_multi(image, reader, conf_th=0.2):\n",
        "    results = []\n",
        "    variants = [\n",
        "        (\"preprocessed\", preprocess_for_arabic_ocr(image)),\n",
        "        (\"original\", image),\n",
        "        (\"grayscale\", cv2.cvtColor(cv2.cvtColor(image, cv2.COLOR_BGR2GRAY), cv2.COLOR_GRAY2BGR)),\n",
        "        (\"enhanced\", cv2.convertScaleAbs(image, alpha=1.5, beta=30)),\n",
        "        (\"inverted\", cv2.bitwise_not(image))\n",
        "    ]\n",
        "    for name, img in variants:\n",
        "        try:\n",
        "            ocr = reader.readtext(img, paragraph=False, width_ths=0.7, height_ths=0.7, detail=1)\n",
        "            for box, text, conf in ocr:\n",
        "                if conf >= conf_th:\n",
        "                    results.append({\"text\": text, \"confidence\": conf, \"approach\": name})\n",
        "        except Exception:\n",
        "            pass\n",
        "    return results\n",
        "\n",
        "plate_model = Path('src/models/yolov8s14/weights/best.pt')\n",
        "detail_model = Path('src/models/yolov8s5/weights/best.pt')\n",
        "example_dir = Path('src/data/plat number car yemen.v1i.yolov8/test/images')\n",
        "\n",
        "if plate_model.exists() and detail_model.exists():\n",
        "    plate = YOLO(str(plate_model))\n",
        "    details = YOLO(str(detail_model))\n",
        "    reader = easyocr.Reader(['en','ar'], gpu=False)\n",
        "    img_path = None\n",
        "    if example_dir.exists():\n",
        "        for p in example_dir.iterdir():\n",
        "            if p.suffix.lower() in {'.jpg','.jpeg','.png'}:\n",
        "                img_path = p; break\n",
        "    if img_path:\n",
        "        img = cv2.imread(str(img_path))\n",
        "        out = []\n",
        "        for r in plate(img):\n",
        "            if r.boxes is None: continue\n",
        "            for b in r.boxes:\n",
        "                x1,y1,x2,y2 = map(int, b.xyxy[0].cpu().numpy())\n",
        "                crop = img[y1:y2, x1:x2]\n",
        "                if crop.size == 0: continue\n",
        "                city, text_ar, num_ar = '', '', ''\n",
        "                for dr in details(crop, conf=0.1):\n",
        "                    if dr.boxes is None: continue\n",
        "                    for db in dr.boxes:\n",
        "                        dx1,dy1,dx2,dy2 = map(int, db.xyxy[0].cpu().numpy())\n",
        "                        cls = int(db.cls[0].cpu().numpy())\n",
        "                        label = dr.names[cls]\n",
        "                        sub = crop[dy1:dy2, dx1:dx2]\n",
        "                        if sub.size == 0: continue\n",
        "                        ocrs = extract_text_multi(sub, reader)\n",
        "                        if ocrs:\n",
        "                            best = max(ocrs, key=lambda x: x['confidence'])\n",
        "                            t = best['text']\n",
        "                            if label == 'city':\n",
        "                                city = t\n",
        "                            elif label == 'text':\n",
        "                                if 'Ø®ØµÙˆØµÙŠ' in t or 'Ù†Ù‚Ù„' in t:\n",
        "                                    t = 'Ø®ØµÙˆØµÙŠ Ù†Ù‚Ù„ Ø§Ø¬Ø±Ø©'\n",
        "                                text_ar = t\n",
        "                            elif label == 'number':\n",
        "                                num_ar = t\n",
        "                out.append({'city': city, 'text': text_ar, 'number_ar': num_ar, 'number_en': arabic_to_english_digits(num_ar) if num_ar else ''})\n",
        "        print(out)\n",
        "    else:\n",
        "        print('No example images found.')\n",
        "else:\n",
        "    print('Required models not found.')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8) Real-time Detection\n",
        "\n",
        "Command-line live stream script:\n",
        "```bash\n",
        "python recognition-with-live-stream/live-stream.py\n",
        "```\n",
        "Edit `STREAM_URL` inside the script.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9) Results and Performance\n",
        "\n",
        "- Trained YOLOv8s model (50 epochs)\n",
        "- Refer to `runs/detect/yolov8n14/results.csv` for metrics (mAP, precision, recall)\n",
        "- Confusion matrices and curves available in `runs/detect/yolov8n14/`\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10) Future Improvements\n",
        "\n",
        "- Expand datasets and annotations for robustness\n",
        "- Improve Arabic OCR post-processing and language models\n",
        "- Export and optimize models for deployment (ONNX/TensorRT)\n",
        "- Add unit tests and CI for training/inference pipelines\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
