{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Car License Plate Detection and Recognition System\n",
        "\n",
        "## Project Overview\n",
        "\n",
        "This project implements an automated car license plate detection and recognition system using computer vision and deep learning techniques. The system is designed to detect license plates from vehicle images or video streams, extract the plate region, and recognize the characters for further processing.\n",
        "\n",
        "### Team Members\n",
        "- **Ahmed Al-Duais** - 202270176\n",
        "- **Abulkareem Thiab** - 202270136\n",
        "- **Ayman Mrwan** - 202270324\n",
        "\n",
        "### Project Goals\n",
        "1. Develop an accurate license plate detection system using YOLO\n",
        "2. Implement OCR-based text recognition for Arabic and English characters\n",
        "3. Create user-friendly GUI applications for real-time processing\n",
        "4. Compare different model architectures and select the optimal one\n",
        "\n",
        "### Technology Stack\n",
        "- **Deep Learning**: YOLOv8 (Ultralytics)\n",
        "- **OCR**: EasyOCR\n",
        "- **GUI Framework**: PySide6 (Qt)\n",
        "- **Computer Vision**: OpenCV\n",
        "- **Data Processing**: NumPy, Pandas\n",
        "- **Visualization**: Matplotlib, Seaborn\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Quick Start and GUI Guide\n",
        "\n",
        "## Setup Instructions\n",
        "\n",
        "- **Python**: 3.13.5+\n",
        "- Install with UV (recommended):\n",
        "```bash\n",
        "pip install uv\n",
        "uv sync\n",
        "```\n",
        "- Verify:\n",
        "```bash\n",
        "python -c \"import ultralytics, easyocr, cv2; print('OK')\"\n",
        "```\n",
        "\n",
        "### Model Setup\n",
        "- Primary model: `src/models/yolov8s14/weights/best.pt`\n",
        "- Component model: `src/models/yolov8s5/weights/best.pt`\n",
        "- Base models: `yolo11n.pt`, `yolo11s.pt`, `yolov8n.pt`, `yolov8s.pt`\n",
        "\n",
        "## GUI Applications\n",
        "\n",
        "### 1) Advanced Qt GUI with Live Stream (recommended)\n",
        "File: `GUIs/qt_with_live/main.py`\n",
        "```bash\n",
        "python GUIs/qt_with_live/main.py\n",
        "```\n",
        "- Dual tabs: Image Recognition + Live Stream\n",
        "- Live Stream: enter URL (http/rtsp/rtmp/tcp), set confidence (0.1–0.9), Start/Stop\n",
        "- Real‑time OCR (Arabic + English)\n",
        "\n",
        "### 2) Simple Qt GUI (single image)\n",
        "File: `GUIs/simple-gui-qt.py`\n",
        "```bash\n",
        "python GUIs/simple-gui-qt.py\n",
        "```\n",
        "- Upload image, detect, OCR with common Arabic fixes\n",
        "\n",
        "### 3) Two‑Stage Recognition GUI\n",
        "File: `yemen_plate_gui.py`\n",
        "```bash\n",
        "python yemen_plate_gui.py\n",
        "```\n",
        "- Stage 1: plate detection (yolov8s14)\n",
        "- Stage 2: components (text/city/number via yolov8s5)\n",
        "- Outputs city, Arabic text, number (with Arabic→English digits)\n",
        "\n",
        "### 4) Enhanced Two‑Stage Recognition GUI (latest)\n",
        "File: `yemen_plate_v2_gui.py`\n",
        "```bash\n",
        "python yemen_plate_v2_gui.py\n",
        "```\n",
        "- Enhanced Arabic OCR: CLAHE, bilateral filter, sharpening, morphology, multi‑approach OCR\n",
        "- Fallback OCR on clean plate if classes fail\n",
        "- Cleans Arabic text, extracts numbers (Arabic/English), converts digits\n",
        "\n",
        "### 5) Simple Tkinter GUI\n",
        "File: `GUIs/simple_gui_tiknter/simple_gui_tiknter.py`\n",
        "```bash\n",
        "python GUIs/simple_gui_tiknter/simple_gui_tiknter.py\n",
        "```\n",
        "- Lightweight upload + detect, status messages, auto model path\n",
        "\n",
        "## Command Line Tools\n",
        "\n",
        "- Live stream: `recognition-with-live-stream/live-stream.py`\n",
        "```bash\n",
        "python recognition-with-live-stream/live-stream.py\n",
        "```\n",
        "Edit `STREAM_URL` before running.\n",
        "\n",
        "- Batch annotate images: `model_test.py`\n",
        "```bash\n",
        "python model_test.py\n",
        "```\n",
        "\n",
        "- Crop plates from images: `seperated-plates.py`\n",
        "```bash\n",
        "python seperated-plates.py\n",
        "```\n",
        "\n",
        "## Project Structure (high level)\n",
        "```\n",
        "src/\n",
        "  data/ (primary + extended datasets)\n",
        "  models/\n",
        "    yolov8s14/ (plate detection)\n",
        "    yolov8s5/  (component detection)\n",
        "GUIs/\n",
        "  qt_with_live/ (tabs + utils)\n",
        "  simple_gui_tiknter/\n",
        "runs/detect/ (training runs, e.g., yolov8n14)\n",
        "```\n",
        "\n",
        "## Additional Features\n",
        "- Two‑stage pipeline (plates → components)\n",
        "- Enhanced Arabic OCR + cleanup + digit conversion\n",
        "- Utilities: `convert_to_yolo.py`, `seperated-plates.py`, `model_test.py`\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Demo 1: Basic detection + OCR\n",
        "import os\n",
        "from pathlib import Path\n",
        "import cv2\n",
        "from ultralytics import YOLO\n",
        "import easyocr\n",
        "\n",
        "# Paths\n",
        "primary_model = Path('src/models/yolov8s14/weights/best.pt')\n",
        "example_dir = Path('src/data/plat number car yemen.v1i.yolov8/test/images')\n",
        "\n",
        "if not primary_model.exists():\n",
        "    print(f\"Model not found: {primary_model}\")\n",
        "else:\n",
        "    model = YOLO(str(primary_model))\n",
        "    reader = easyocr.Reader(['en','ar'])\n",
        "\n",
        "    if example_dir.exists():\n",
        "        # pick first image as example\n",
        "        try:\n",
        "            img_path = next(p for p in example_dir.iterdir() if p.suffix.lower() in {'.jpg','.jpeg','.png'})\n",
        "        except StopIteration:\n",
        "            img_path = None\n",
        "    else:\n",
        "        img_path = None\n",
        "\n",
        "    if img_path is None:\n",
        "        print(\"No example images found.\")\n",
        "    else:\n",
        "        img = cv2.imread(str(img_path))\n",
        "        results = model(img)\n",
        "\n",
        "        detections = []\n",
        "        for r in results:\n",
        "            if r.boxes is None:\n",
        "                continue\n",
        "            for box in r.boxes:\n",
        "                x1, y1, x2, y2 = map(int, box.xyxy[0].cpu().numpy())\n",
        "                conf = float(box.conf[0])\n",
        "                detections.append(((x1,y1,x2,y2), conf))\n",
        "\n",
        "        print(f\"Detected {len(detections)} plate(s) in {img_path.name}\")\n",
        "\n",
        "        for i, (bbox, conf) in enumerate(detections, 1):\n",
        "            x1,y1,x2,y2 = bbox\n",
        "            crop = img[y1:y2, x1:x2]\n",
        "            if crop.size == 0:\n",
        "                continue\n",
        "            ocr = reader.readtext(crop)\n",
        "            text = \" \".join([t[1] for t in ocr])\n",
        "            # simple Arabic fix\n",
        "            if 'خصوصي' in text or 'نقل' in text:\n",
        "                text = 'خصوصي نقل اجرة'\n",
        "            print(f\"Plate {i}: conf={conf:.2f} text='{text}'\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Demo 2: Two-stage detection (plates → components) + enhanced OCR\n",
        "import os\n",
        "from pathlib import Path\n",
        "import cv2\n",
        "from ultralytics import YOLO\n",
        "import easyocr\n",
        "import numpy as np\n",
        "\n",
        "def arabic_to_english_digits(text: str) -> str:\n",
        "    return text.translate(str.maketrans(\"٠١٢٣٤٥٦٧٨٩\", \"0123456789\"))\n",
        "\n",
        "def preprocess_for_arabic_ocr(image):\n",
        "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "    h, w = gray.shape\n",
        "    if h < 50 or w < 100:\n",
        "        sf = max(50/h, 100/w, 2.0)\n",
        "        gray = cv2.resize(gray, (int(w*sf), int(h*sf)), interpolation=cv2.INTER_CUBIC)\n",
        "    blurred = cv2.GaussianBlur(gray, (3,3), 0)\n",
        "    clahe = cv2.createCLAHE(clipLimit=4.0, tileGridSize=(8,8))\n",
        "    enhanced = clahe.apply(blurred)\n",
        "    filtered = cv2.bilateralFilter(enhanced, 9, 75, 75)\n",
        "    kernel = np.array([[-1,-1,-1],[-1,9,-1],[-1,-1,-1]])\n",
        "    sharpened = cv2.filter2D(filtered, -1, kernel)\n",
        "    thresh = cv2.adaptiveThreshold(sharpened, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 11, 2)\n",
        "    return cv2.cvtColor(thresh, cv2.COLOR_GRAY2BGR)\n",
        "\n",
        "def extract_text_multi(image, reader, conf_th=0.2):\n",
        "    results = []\n",
        "    variants = [\n",
        "        (\"preprocessed\", preprocess_for_arabic_ocr(image)),\n",
        "        (\"original\", image),\n",
        "        (\"grayscale\", cv2.cvtColor(cv2.cvtColor(image, cv2.COLOR_BGR2GRAY), cv2.COLOR_GRAY2BGR)),\n",
        "        (\"enhanced\", cv2.convertScaleAbs(image, alpha=1.5, beta=30)),\n",
        "        (\"inverted\", cv2.bitwise_not(image))\n",
        "    ]\n",
        "    for name, img in variants:\n",
        "        try:\n",
        "            ocr = reader.readtext(img, paragraph=False, width_ths=0.7, height_ths=0.7, detail=1)\n",
        "            for box, text, conf in ocr:\n",
        "                if conf >= conf_th:\n",
        "                    results.append({\"text\": text, \"confidence\": conf, \"approach\": name})\n",
        "        except Exception as e:\n",
        "            pass\n",
        "    return results\n",
        "\n",
        "# Paths\n",
        "plate_model_path = Path('src/models/yolov8s14/weights/best.pt')\n",
        "detail_model_path = Path('src/models/yolov8s5/weights/best.pt')\n",
        "example_dir = Path('src/data/plat number car yemen.v1i.yolov8/test/images')\n",
        "\n",
        "if not plate_model_path.exists() or not detail_model_path.exists():\n",
        "    print(\"Required models not found.\")\n",
        "else:\n",
        "    plate_model = YOLO(str(plate_model_path))\n",
        "    detail_model = YOLO(str(detail_model_path))\n",
        "    reader = easyocr.Reader(['en','ar'], gpu=False)\n",
        "\n",
        "    if example_dir.exists():\n",
        "        try:\n",
        "            img_path = next(p for p in example_dir.iterdir() if p.suffix.lower() in {'.jpg','.jpeg','.png'})\n",
        "        except StopIteration:\n",
        "            img_path = None\n",
        "    else:\n",
        "        img_path = None\n",
        "\n",
        "    if img_path is None:\n",
        "        print(\"No example images found.\")\n",
        "    else:\n",
        "        img = cv2.imread(str(img_path))\n",
        "        all_results = []\n",
        "        plate_results = plate_model(img)\n",
        "        for r in plate_results:\n",
        "            if r.boxes is None:\n",
        "                continue\n",
        "            for box in r.boxes:\n",
        "                x1, y1, x2, y2 = map(int, box.xyxy[0].cpu().numpy())\n",
        "                plate_crop = img[y1:y2, x1:x2]\n",
        "                if plate_crop.size == 0:\n",
        "                    continue\n",
        "                city_val, arabic_text, arabic_number = \"\", \"\", \"\"\n",
        "                details = detail_model(plate_crop, conf=0.1)\n",
        "                for dr in details:\n",
        "                    if dr.boxes is None:\n",
        "                        continue\n",
        "                    for dbox in dr.boxes:\n",
        "                        dx1, dy1, dx2, dy2 = map(int, dbox.xyxy[0].cpu().numpy())\n",
        "                        cls_id = int(dbox.cls[0].cpu().numpy())\n",
        "                        label = dr.names[cls_id]\n",
        "                        crop = plate_crop[dy1:dy2, dx1:dx2]\n",
        "                        if crop.size == 0:\n",
        "                            continue\n",
        "                        ocr_best = extract_text_multi(crop, reader)\n",
        "                        if ocr_best:\n",
        "                            best = max(ocr_best, key=lambda x: x['confidence'])\n",
        "                            text = best['text']\n",
        "                            if label == 'city':\n",
        "                                city_val = text\n",
        "                            elif label == 'text':\n",
        "                                if 'خصوصي' in text or 'نقل' in text:\n",
        "                                    text = 'خصوصي نقل اجرة'\n",
        "                                arabic_text = text\n",
        "                            elif label == 'number':\n",
        "                                arabic_number = text\n",
        "                all_results.append({\n",
        "                    'city': city_val,\n",
        "                    'text': arabic_text,\n",
        "                    'number_ar': arabic_number,\n",
        "                    'number_en': arabic_to_english_digits(arabic_number) if arabic_number else ''\n",
        "                })\n",
        "        if all_results:\n",
        "            print(all_results)\n",
        "        else:\n",
        "            print(\"No plates detected.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import os\n",
        "import sys\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from pathlib import Path\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set up plotting style\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "# Configure matplotlib for better plots\n",
        "plt.rcParams['figure.figsize'] = (12, 8)\n",
        "plt.rcParams['font.size'] = 12\n",
        "\n",
        "print(\"Libraries imported successfully!\")\n",
        "print(f\"OpenCV version: {cv2.__version__}\")\n",
        "print(f\"NumPy version: {np.__version__}\")\n",
        "print(f\"Pandas version: {pd.__version__}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Dataset Analysis and Exploration\n",
        "\n",
        "### Dataset Structure\n",
        "\n",
        "Our project utilizes two main datasets for training and validation:\n",
        "\n",
        "1. **Primary Dataset**: `plat number car yemen.v1i.yolov8`\n",
        "   - Single class: \"private\" \n",
        "   - 80 training images, 16 validation images, 8 test images\n",
        "   - Focus on private vehicle license plates\n",
        "\n",
        "2. **Extended Dataset**: `yemen-plate`\n",
        "   - Three classes: \"city\", \"number\", \"text\"\n",
        "   - 52 training images, 52 validation images, 52 test images\n",
        "   - More detailed annotation for different plate components\n",
        "\n",
        "Let's explore the dataset structure and characteristics.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Dataset exploration\n",
        "def explore_dataset(dataset_path, dataset_name):\n",
        "    \"\"\"Explore dataset structure and provide statistics\"\"\"\n",
        "    print(f\"\\n=== {dataset_name} Dataset Analysis ===\")\n",
        "    \n",
        "    # Check if dataset exists\n",
        "    if not os.path.exists(dataset_path):\n",
        "        print(f\"Dataset not found at: {dataset_path}\")\n",
        "        return\n",
        "    \n",
        "    # Count images in each split\n",
        "    splits = ['train', 'valid', 'test']\n",
        "    total_images = 0\n",
        "    \n",
        "    for split in splits:\n",
        "        split_path = os.path.join(dataset_path, split, 'images')\n",
        "        if os.path.exists(split_path):\n",
        "            image_count = len([f for f in os.listdir(split_path) if f.lower().endswith(('.jpg', '.jpeg', '.png'))])\n",
        "            print(f\"{split.capitalize()} images: {image_count}\")\n",
        "            total_images += image_count\n",
        "        else:\n",
        "            print(f\"{split.capitalize()} images: 0 (directory not found)\")\n",
        "    \n",
        "    print(f\"Total images: {total_images}\")\n",
        "    \n",
        "    # Check for labels\n",
        "    for split in splits:\n",
        "        label_path = os.path.join(dataset_path, split, 'labels')\n",
        "        if os.path.exists(label_path):\n",
        "            label_count = len([f for f in os.listdir(label_path) if f.endswith('.txt')])\n",
        "            print(f\"{split.capitalize()} labels: {label_count}\")\n",
        "    \n",
        "    # Read data.yaml if exists\n",
        "    yaml_path = os.path.join(dataset_path, 'data.yaml')\n",
        "    if os.path.exists(yaml_path):\n",
        "        print(f\"\\nDataset configuration found: {yaml_path}\")\n",
        "        with open(yaml_path, 'r') as f:\n",
        "            print(f.read())\n",
        "\n",
        "# Explore both datasets\n",
        "datasets = {\n",
        "    \"Primary Dataset\": \"src/data/plat number car yemen.v1i.yolov8\",\n",
        "    \"Extended Dataset\": \"src/data/yemen-plate\"\n",
        "}\n",
        "\n",
        "for name, path in datasets.items():\n",
        "    explore_dataset(path, name)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize sample images from the dataset\n",
        "def visualize_sample_images(dataset_path, dataset_name, num_samples=4):\n",
        "    \"\"\"Display sample images from the dataset\"\"\"\n",
        "    print(f\"\\n=== Sample Images from {dataset_name} ===\")\n",
        "    \n",
        "    # Get sample images from training set\n",
        "    train_images_path = os.path.join(dataset_path, 'train', 'images')\n",
        "    if not os.path.exists(train_images_path):\n",
        "        print(f\"Training images not found at: {train_images_path}\")\n",
        "        return\n",
        "    \n",
        "    image_files = [f for f in os.listdir(train_images_path) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
        "    \n",
        "    if len(image_files) == 0:\n",
        "        print(\"No images found in training directory\")\n",
        "        return\n",
        "    \n",
        "    # Select random samples\n",
        "    import random\n",
        "    sample_files = random.sample(image_files, min(num_samples, len(image_files)))\n",
        "    \n",
        "    # Create subplot\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "    axes = axes.ravel()\n",
        "    \n",
        "    for i, filename in enumerate(sample_files):\n",
        "        if i >= 4:\n",
        "            break\n",
        "            \n",
        "        img_path = os.path.join(train_images_path, filename)\n",
        "        img = cv2.imread(img_path)\n",
        "        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "        \n",
        "        axes[i].imshow(img_rgb)\n",
        "        axes[i].set_title(f\"Sample {i+1}: {filename[:30]}...\")\n",
        "        axes[i].axis('off')\n",
        "    \n",
        "    # Hide unused subplots\n",
        "    for i in range(len(sample_files), 4):\n",
        "        axes[i].axis('off')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Visualize samples from both datasets\n",
        "for name, path in datasets.items():\n",
        "    visualize_sample_images(path, name)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Model Development and Training\n",
        "\n",
        "### YOLO Model Architecture Selection\n",
        "\n",
        "Our project involved testing different YOLO model architectures to find the optimal balance between accuracy and performance:\n",
        "\n",
        "1. **YOLOv8n (Nano)**: Initial attempt with lightweight model\n",
        "   - Fast inference but insufficient accuracy\n",
        "   - Results stored in `runs/detect/detect_nano/`\n",
        "\n",
        "2. **YOLOv8s (Small)**: Final selected model\n",
        "   - Better accuracy while maintaining reasonable speed\n",
        "   - 50 epochs training with comprehensive evaluation\n",
        "   - Results stored in `runs/detect/yolov8n14/`\n",
        "\n",
        "### Training Configuration\n",
        "\n",
        "The final model was trained with the following parameters:\n",
        "- **Model**: YOLOv8s (small)\n",
        "- **Epochs**: 50\n",
        "- **Image Size**: 640x640\n",
        "- **Batch Size**: 9\n",
        "- **Optimizer**: Auto (AdamW)\n",
        "- **Learning Rate**: 0.01 (with cosine annealing)\n",
        "- **Data Augmentation**: Enabled (mosaic, mixup, cutmix)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load and analyze training results\n",
        "def load_training_results(results_path):\n",
        "    \"\"\"Load and analyze YOLO training results\"\"\"\n",
        "    results_file = os.path.join(results_path, 'results.csv')\n",
        "    \n",
        "    if not os.path.exists(results_file):\n",
        "        print(f\"Results file not found: {results_file}\")\n",
        "        return None\n",
        "    \n",
        "    # Load results\n",
        "    df = pd.read_csv(results_file)\n",
        "    \n",
        "    print(f\"Training completed in {len(df)} epochs\")\n",
        "    print(f\"Total training time: {df['time'].iloc[-1]:.2f} seconds ({df['time'].iloc[-1]/3600:.2f} hours)\")\n",
        "    \n",
        "    return df\n",
        "\n",
        "# Load training results\n",
        "results_path = \"runs/detect/yolov8n14\"\n",
        "training_results = load_training_results(results_path)\n",
        "\n",
        "if training_results is not None:\n",
        "    print(\"\\n=== Training Results Summary ===\")\n",
        "    print(f\"Final mAP50: {training_results['metrics/mAP50(B)'].iloc[-1]:.4f}\")\n",
        "    print(f\"Final mAP50-95: {training_results['metrics/mAP50-95(B)'].iloc[-1]:.4f}\")\n",
        "    print(f\"Final Precision: {training_results['metrics/precision(B)'].iloc[-1]:.4f}\")\n",
        "    print(f\"Final Recall: {training_results['metrics/recall(B)'].iloc[-1]:.4f}\")\n",
        "    print(f\"Final Box Loss: {training_results['train/box_loss'].iloc[-1]:.4f}\")\n",
        "    print(f\"Final Class Loss: {training_results['train/cls_loss'].iloc[-1]:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize training metrics\n",
        "def plot_training_metrics(df):\n",
        "    \"\"\"Plot training metrics over epochs\"\"\"\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "    \n",
        "    # Plot 1: Loss curves\n",
        "    axes[0, 0].plot(df['epoch'], df['train/box_loss'], label='Box Loss', color='blue')\n",
        "    axes[0, 0].plot(df['epoch'], df['train/cls_loss'], label='Class Loss', color='red')\n",
        "    axes[0, 0].plot(df['epoch'], df['train/dfl_loss'], label='DFL Loss', color='green')\n",
        "    axes[0, 0].set_title('Training Losses')\n",
        "    axes[0, 0].set_xlabel('Epoch')\n",
        "    axes[0, 0].set_ylabel('Loss')\n",
        "    axes[0, 0].legend()\n",
        "    axes[0, 0].grid(True)\n",
        "    \n",
        "    # Plot 2: Validation losses\n",
        "    axes[0, 1].plot(df['epoch'], df['val/box_loss'], label='Val Box Loss', color='blue')\n",
        "    axes[0, 1].plot(df['epoch'], df['val/cls_loss'], label='Val Class Loss', color='red')\n",
        "    axes[0, 1].plot(df['epoch'], df['val/dfl_loss'], label='Val DFL Loss', color='green')\n",
        "    axes[0, 1].set_title('Validation Losses')\n",
        "    axes[0, 1].set_xlabel('Epoch')\n",
        "    axes[0, 1].set_ylabel('Loss')\n",
        "    axes[0, 1].legend()\n",
        "    axes[0, 1].grid(True)\n",
        "    \n",
        "    # Plot 3: Precision and Recall\n",
        "    axes[1, 0].plot(df['epoch'], df['metrics/precision(B)'], label='Precision', color='purple')\n",
        "    axes[1, 0].plot(df['epoch'], df['metrics/recall(B)'], label='Recall', color='orange')\n",
        "    axes[1, 0].set_title('Precision and Recall')\n",
        "    axes[1, 0].set_xlabel('Epoch')\n",
        "    axes[1, 0].set_ylabel('Score')\n",
        "    axes[1, 0].legend()\n",
        "    axes[1, 0].grid(True)\n",
        "    \n",
        "    # Plot 4: mAP scores\n",
        "    axes[1, 1].plot(df['epoch'], df['metrics/mAP50(B)'], label='mAP@0.5', color='red')\n",
        "    axes[1, 1].plot(df['epoch'], df['metrics/mAP50-95(B)'], label='mAP@0.5:0.95', color='blue')\n",
        "    axes[1, 1].set_title('Mean Average Precision')\n",
        "    axes[1, 1].set_xlabel('Epoch')\n",
        "    axes[1, 1].set_ylabel('mAP')\n",
        "    axes[1, 1].legend()\n",
        "    axes[1, 1].grid(True)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "if training_results is not None:\n",
        "    plot_training_metrics(training_results)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Model Implementation and Architecture\n",
        "\n",
        "### Core Components\n",
        "\n",
        "Our license plate recognition system consists of three main components:\n",
        "\n",
        "1. **Plate Detector**: YOLO-based object detection for locating license plates\n",
        "2. **Plate Recognizer**: OCR-based text extraction using EasyOCR\n",
        "3. **GUI Application**: PySide6-based user interface for real-time processing\n",
        "\n",
        "### Plate Detection Module\n",
        "\n",
        "The `PlateDetector` class handles the detection of license plates in images:\n",
        "\n",
        "```python\n",
        "class PlateDetector:\n",
        "    def __init__(self, model_path):\n",
        "        self.model = YOLO(model_path)\n",
        "    \n",
        "    def detect(self, image, conf_threshold=0.5):\n",
        "        \"\"\"Detect license plates in an image\"\"\"\n",
        "        results = self.model(image)\n",
        "        detections = []\n",
        "        \n",
        "        for r in results:\n",
        "            for box in r.boxes:\n",
        "                conf = float(box.conf[0])\n",
        "                if conf >= conf_threshold:\n",
        "                    x1, y1, x2, y2 = map(int, box.xyxy[0].cpu().numpy())\n",
        "                    detections.append({\n",
        "                        'bbox': (x1, y1, x2, y2),\n",
        "                        'confidence': conf,\n",
        "                        'crop': image[y1:y2, x1:x2]\n",
        "                    })\n",
        "        \n",
        "        return detections\n",
        "```\n",
        "\n",
        "### Text Recognition Module\n",
        "\n",
        "The `PlateRecognizer` class handles OCR for Arabic and English text:\n",
        "\n",
        "```python\n",
        "class PlateRecognizer:\n",
        "    def __init__(self, languages=['en', 'ar']):\n",
        "        self.reader = easyocr.Reader(languages)\n",
        "    \n",
        "    def recognize(self, image):\n",
        "        \"\"\"Recognize text from a license plate image\"\"\"\n",
        "        ocr_results = self.reader.readtext(image)\n",
        "        text_detected = \" \".join([res[1] for res in ocr_results])\n",
        "        \n",
        "        # Fix common misread words\n",
        "        if \"خصوصي\" in text_detected or \"نقل\" in text_detected:\n",
        "            text_detected = \"خصوصي نقل اجرة\"\n",
        "            \n",
        "        return text_detected\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test the model on sample images\n",
        "def test_model_on_samples():\n",
        "    \"\"\"Test the trained model on sample images\"\"\"\n",
        "    try:\n",
        "        from ultralytics import YOLO\n",
        "        import easyocr\n",
        "        \n",
        "        # Load the trained model\n",
        "        model_path = \"src/models/yolov8s14/weights/best.pt\"\n",
        "        if not os.path.exists(model_path):\n",
        "            print(f\"Model not found at: {model_path}\")\n",
        "            return\n",
        "        \n",
        "        model = YOLO(model_path)\n",
        "        print(\"Model loaded successfully!\")\n",
        "        \n",
        "        # Load OCR\n",
        "        reader = easyocr.Reader(['en', 'ar'])\n",
        "        print(\"OCR reader initialized!\")\n",
        "        \n",
        "        # Test on sample images\n",
        "        test_images_path = \"src/data/plat number car yemen.v1i.yolov8/test/images\"\n",
        "        if os.path.exists(test_images_path):\n",
        "            image_files = [f for f in os.listdir(test_images_path) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
        "            \n",
        "            if len(image_files) > 0:\n",
        "                # Test on first image\n",
        "                test_image = os.path.join(test_images_path, image_files[0])\n",
        "                print(f\"\\nTesting on: {image_files[0]}\")\n",
        "                \n",
        "                # Load and process image\n",
        "                img = cv2.imread(test_image)\n",
        "                img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "                \n",
        "                # Detect plates\n",
        "                results = model(img)\n",
        "                \n",
        "                # Process results\n",
        "                detections = []\n",
        "                for r in results:\n",
        "                    for box in r.boxes:\n",
        "                        conf = float(box.conf[0])\n",
        "                        if conf >= 0.5:  # Confidence threshold\n",
        "                            x1, y1, x2, y2 = map(int, box.xyxy[0].cpu().numpy())\n",
        "                            detections.append({\n",
        "                                'bbox': (x1, y1, x2, y2),\n",
        "                                'confidence': conf,\n",
        "                                'crop': img[y1:y2, x1:x2]\n",
        "                            })\n",
        "                \n",
        "                print(f\"Found {len(detections)} license plates\")\n",
        "                \n",
        "                # Display results\n",
        "                fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
        "                \n",
        "                # Original image\n",
        "                axes[0].imshow(img_rgb)\n",
        "                axes[0].set_title(\"Original Image\")\n",
        "                axes[0].axis('off')\n",
        "                \n",
        "                # Annotated image\n",
        "                annotated = img_rgb.copy()\n",
        "                for i, det in enumerate(detections):\n",
        "                    x1, y1, x2, y2 = det['bbox']\n",
        "                    cv2.rectangle(annotated, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
        "                    cv2.putText(annotated, f\"Plate {i+1}: {det['confidence']:.2f}\", \n",
        "                               (x1, y1-10), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
        "                \n",
        "                axes[1].imshow(annotated)\n",
        "                axes[1].set_title(\"Detection Results\")\n",
        "                axes[1].axis('off')\n",
        "                \n",
        "                plt.tight_layout()\n",
        "                plt.show()\n",
        "                \n",
        "                # Test OCR on detected plates\n",
        "                for i, det in enumerate(detections):\n",
        "                    if det['crop'].size > 0:\n",
        "                        try:\n",
        "                            ocr_results = reader.readtext(det['crop'])\n",
        "                            text_detected = \" \".join([res[1] for res in ocr_results])\n",
        "                            print(f\"Plate {i+1} text: {text_detected}\")\n",
        "                        except Exception as e:\n",
        "                            print(f\"OCR failed for plate {i+1}: {e}\")\n",
        "            else:\n",
        "                print(\"No test images found\")\n",
        "        else:\n",
        "            print(\"Test images directory not found\")\n",
        "            \n",
        "    except ImportError as e:\n",
        "        print(f\"Required libraries not available: {e}\")\n",
        "        print(\"Please install ultralytics and easyocr to test the model\")\n",
        "\n",
        "# Test the model\n",
        "test_model_on_samples()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
